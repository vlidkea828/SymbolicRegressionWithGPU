"""Class containing equations."""
import numpy as np
import geppy as gep
import sys

from ..Utilities import globals

from ..Utilities.ExtraEquations import (
    new_attacker_value,
    add_all,
    protected_div,
    old_health_value
)

class EvaluationModel:
    """Model for evaluating methods"""
    def add_func(self, func):
        self.func = func
    
class EvaluationModelTest(EvaluationModel):
    """Model for evaluating methods"""
    def evaluate_func(self, A, AL, P, D, DL, H):
        return self.func(A, AL, P, D, DL, H)
    
class EvaluationModelAttackerValue(EvaluationModel):
    """Model for evaluating methods"""
    def evaluate_func(self, A, AL):
        return self.func(A, AL)
    
class EvaluationModelAttackerDefenderValue(EvaluationModel):
    """Model for evaluating methods"""
    def evaluate_func(self, A, AL, D, DL):
        return new_attacker_value(A, AL) - self.func(D, DL)
    
class EvaluationModelAllValues(EvaluationModel):
    """Model for evaluating methods"""
    def evaluate_func(self, A, AL, P, D, DL, H):
        attack = self.func[0](A, AL)
        attacker_level = self.func[1](AL, AL)
        power = self.func[2](P, P)
        defense = self.func[3](D, DL)
        defender_level = self.func[4](DL, DL)
        health = self.func[5](H, DL)
        return protected_div(attack + attacker_level + power - defense - defender_level, health)
    
class EvaluationModelAllValuesNoHealth(EvaluationModel):
    """Model for evaluating methods"""
    def evaluate_func(self, A, AL, P, D, DL):
        attack = self.func[0](A, AL)
        attacker_level = self.func[1](AL, AL)
        power = self.func[2](P, P)
        defense = self.func[3](D, DL)
        defender_level = self.func[4](DL, DL)
        return attack + attacker_level + power - defense - defender_level
    
class EvaluationModelAllValuesWithHealth(EvaluationModel):
    """Model for evaluating methods"""
    def evaluate_func(self, A, AL, P, D, DL, H):
        attack = self.func[0](A, AL)
        attacker_level = self.func[1](AL, AL)
        power = self.func[2](P, P)
        defense = self.func[3](D, DL)
        defender_level = self.func[4](DL, DL)
        health = old_health_value(H, DL)
        return protected_div(attack + attacker_level + power - defense - defender_level, health)

def _compile_gene(g, pset):
    """
    Compile one gene *g* with the primitive set *pset*.
    :return: a function or an evaluated result
    """
    code = str(g)
    if len(pset.input_names) > 0:   # form a Lambda function
        args = ', '.join(pset.input_names)
        code =  'lambda {}: {}'.format(args, code)
    # evaluate the code
    try:
        return eval(code, pset.globals, {})
    except MemoryError:
        _, _, traceback = sys.exc_info()
        raise MemoryError("The expression tree generated by GEP is too deep. Python cannot evaluate a tree higher "
                          "than 90. You should try to adopt a smaller head length for the genes, for example, by using"
                          "more genes in a chromosome.").with_traceback(traceback)

class EvaluationMethod:
    """Defenition of a node graph."""
    def __init__(
            self,
            toolbox,
            inputs,
            results,
            mapping_function,
            evaluation_model,
            pset
    ):
        self.toolbox = toolbox
        self.inputs = inputs
        self.results = results
        self.mapping_function = mapping_function
        self.evaluation_model = evaluation_model
        self.pset = pset

    # @jit
    def evaluate(self, individual):
        """Evalute the fitness of an individual: MAE (mean absolute error)"""
        func = self.toolbox.compile(individual)
        self.evaluation_model.add_func(func)
        
        # below call the individual as a function over the inputs
        
        # Yp = np.array(list(map(func, X)))
        Rp = np.array(list(self.mapping_function(self.evaluation_model.evaluate_func, self.inputs))) 
        
        # return the MSE as we are evaluating on it anyway - then the stats are more fun to watch...
        return np.mean((self.results - Rp) ** 2),

    # @jit
    def evaluate_ls(self, individual):
        """
        First apply linear scaling (ls) to the individual 
        and then evaluate its fitness: MSE (mean squared error)
        """
        func = self.toolbox.compile(individual)
        #... add tests between the 6 funcs and the main
        self.evaluation_model.add_func(func)
        Rp = np.array(list(self.mapping_function(self.evaluation_model.evaluate_func, self.inputs)))
        
        # special cases which cannot be handled by np.linalg.lstsq: (1) individual has only a terminal 
        #  (2) individual returns the same value for all test cases, like 'x - x + 10'. np.linalg.lstsq will fail in such cases.
        # That is, the predicated value for all the examples remains identical, which may happen in the evolution.
        if isinstance(Rp, np.ndarray):
            Q = np.hstack((np.reshape(Rp, (-1, 1)), np.ones((len(Rp), 1))))
            (individual.a, individual.b), residuals, _, _ = np.linalg.lstsq(Q, self.results)   
            # residuals is the sum of squared errors
            if residuals.size > 0:
                return residuals[0] / len(self.results),   # MSE
        
        # regarding the above special cases, the optimal linear scaling w.r.t LSM is just the mean of true target values
        individual.a = 0
        individual.b = np.mean(self.results)

        result = np.mean((self.results - individual.b) ** 2),

        return result
    
    # @jit
    def evaluate_ls_new(self, individual):
        """
        First apply linear scaling (ls) to the individual 
        and then evaluate its fitness: MSE (mean squared error)
        """
        func1 = _compile_gene(individual[0], self.pset)
        func2 = _compile_gene(individual[1], self.pset)
        func3 = _compile_gene(individual[2], self.pset)
        func4 = _compile_gene(individual[3], self.pset)
        func5 = _compile_gene(individual[4], self.pset)
        func6 = _compile_gene(individual[5], self.pset)
        self.evaluation_model.add_func([
            func1,
            func2,
            func3,
            func4,
            func5,
            func6])
        Rp = np.array(list(self.mapping_function(self.evaluation_model.evaluate_func, self.inputs)))
        
        # special cases which cannot be handled by np.linalg.lstsq: (1) individual has only a terminal 
        #  (2) individual returns the same value for all test cases, like 'x - x + 10'. np.linalg.lstsq will fail in such cases.
        # That is, the predicated value for all the examples remains identical, which may happen in the evolution.
        if isinstance(Rp, np.ndarray):
            Q = np.hstack((np.reshape(Rp, (-1, 1)), np.ones((len(Rp), 1))))
            (individual.a, individual.b), residuals, _, _ = np.linalg.lstsq(Q, self.results)   
            # residuals is the sum of squared errors
            if residuals.size > 0:
                return residuals[0] / len(self.results),   # MSE
        
        # regarding the above special cases, the optimal linear scaling w.r.t LSM is just the mean of true target values
        individual.a = 0
        individual.b = np.mean(self.results)

        result = np.mean((self.results - individual.b) ** 2),

        return result
    
    # @jit
    def evaluate_ls_new_two(self, individual):
        """
        First apply linear scaling (ls) to the individual 
        and then evaluate its fitness: MSE (mean squared error)
        """
        func1 = _compile_gene(individual[0], self.pset)
        func2 = _compile_gene(individual[1], self.pset)
        func3 = _compile_gene(individual[2], self.pset)
        func4 = _compile_gene(individual[3], self.pset)
        func5 = _compile_gene(individual[4], self.pset)
        self.evaluation_model.add_func([
            func1,
            func2,
            func3,
            func4,
            func5])
        Rp = np.array(list(self.mapping_function(self.evaluation_model.evaluate_func, self.inputs)))
        
        # special cases which cannot be handled by np.linalg.lstsq: (1) individual has only a terminal 
        #  (2) individual returns the same value for all test cases, like 'x - x + 10'. np.linalg.lstsq will fail in such cases.
        # That is, the predicated value for all the examples remains identical, which may happen in the evolution.
        if isinstance(Rp, np.ndarray):
            Q = np.hstack((np.reshape(Rp, (-1, 1)), np.ones((len(Rp), 1))))
            (individual.a, individual.b), residuals, _, _ = np.linalg.lstsq(Q, self.results)   
            # residuals is the sum of squared errors
            if residuals.size > 0:
                return residuals[0] / len(self.results),   # MSE
        
        # regarding the above special cases, the optimal linear scaling w.r.t LSM is just the mean of true target values
        individual.a = 0
        individual.b = np.mean(self.results)

        result = np.mean((self.results - individual.b) ** 2),

        return result

def mapping_function(func, inputs):
    return map(
        func,
        inputs[0],
        inputs[1],
        inputs[2],
        inputs[3],
        inputs[4],
        inputs[5])

def newest_evaluate(individual):
    """
    First apply linear scaling (ls) to the individual 
    and then evaluate its fitness: MSE (mean squared error)
    """
    func1 = _compile_gene(individual[0], globals.pset)
    func2 = _compile_gene(individual[1], globals.pset)
    func3 = _compile_gene(individual[2], globals.pset)
    func4 = _compile_gene(individual[3], globals.pset)
    func5 = _compile_gene(individual[4], globals.pset)
    evaluate_func=lambda A, AL, P, D, DL, H : protected_div(
        func1(A, AL) +\
        func2(AL, AL) +\
        func3(P, P) -\
        func4(D, DL) -\
        func5(DL, DL),
        old_health_value(H, DL)
    )

    Rp = np.array(list(mapping_function(evaluate_func, globals.inputs)))
    
    # special cases which cannot be handled by np.linalg.lstsq: (1) individual has only a terminal 
    #  (2) individual returns the same value for all test cases, like 'x - x + 10'. np.linalg.lstsq will fail in such cases.
    # That is, the predicated value for all the examples remains identical, which may happen in the evolution.
    if isinstance(Rp, np.ndarray):
        Q = np.hstack((np.reshape(Rp, (-1, 1)), np.ones((len(Rp), 1))))
        (individual.a, individual.b), residuals, _, _ = np.linalg.lstsq(Q, globals.results)   
        # residuals is the sum of squared errors
        if residuals.size > 0:
            return residuals[0] / len(globals.results),   # MSE
    
    # regarding the above special cases, the optimal linear scaling w.r.t LSM is just the mean of true target values
    individual.a = 0
    individual.b = np.mean(globals.results)

    result = np.mean((globals.results - individual.b) ** 2),

    return result
